{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f9c39589bddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualization_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvis_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-051a1e55f8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_tool\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloc2bbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from utils import array_tool as at\n",
    "from model.utils.bbox_tools import loc2bbox\n",
    "from model.utils.nms import non_maximum_suppression\n",
    "\n",
    "from torch import nn\n",
    "from data.dataset import preprocess\n",
    "from torch.nn import functional as F\n",
    "from utils.config import opt\n",
    "\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    \"\"\"Base class for Faster R-CNN.\n",
    "    This is a base class for Faster R-CNN links supporting object detection\n",
    "    API [#]_. The following three stages constitute Faster R-CNN.\n",
    "    1. **Feature extraction**: Images are taken and their \\\n",
    "        feature maps are calculated.\n",
    "    2. **Region Proposal Networks**: Given the feature maps calculated in \\\n",
    "        the previous stage, produce set of RoIs around objects.\n",
    "    3. **Localization and Classification Heads**: Using feature maps that \\\n",
    "        belong to the proposed RoIs, classify the categories of the objects \\\n",
    "        in the RoIs and improve localizations.\n",
    "    Each stage is carried out by one of the callable\n",
    "    :class:`torch.nn.Module` objects :obj:`feature`, :obj:`rpn` and :obj:`head`.\n",
    "    There are two functions :meth:`predict` and :meth:`__call__` to conduct\n",
    "    object detection.\n",
    "    :meth:`predict` takes images and returns bounding boxes that are converted\n",
    "    to image coordinates. This will be useful for a scenario when\n",
    "    Faster R-CNN is treated as a black box function, for instance.\n",
    "    :meth:`__call__` is provided for a scnerario when intermediate outputs\n",
    "    are needed, for instance, for training and debugging.\n",
    "    Links that support obejct detection API have method :meth:`predict` with\n",
    "    the same interface. Please refer to :meth:`predict` for\n",
    "    further details.\n",
    "    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n",
    "    Faster R-CNN: Towards Real-Time Object Detection with \\\n",
    "    Region Proposal Networks. NIPS 2015.\n",
    "    Args:\n",
    "        extractor (nn.Module): A module that takes a BCHW image\n",
    "            array and returns feature maps.\n",
    "        rpn (nn.Module): A module that has the same interface as\n",
    "            :class:`model.region_proposal_network.RegionProposalNetwork`.\n",
    "            Please refer to the documentation found there.\n",
    "        head (nn.Module): A module that takes\n",
    "            a BCHW variable, RoIs and batch indices for RoIs. This returns class\n",
    "            dependent localization paramters and class scores.\n",
    "        loc_normalize_mean (tuple of four floats): Mean values of\n",
    "            localization estimates.\n",
    "        loc_normalize_std (tupler of four floats): Standard deviation\n",
    "            of localization estimates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, extractor, rpn, head,\n",
    "                loc_normalize_mean = (0., 0., 0., 0.),\n",
    "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)\n",
    "    ):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.extractor = extractor\n",
    "        self.rpn = rpn\n",
    "        self.head = head\n",
    "\n",
    "        # mean and std\n",
    "        self.loc_normalize_mean = loc_normalize_mean\n",
    "        self.loc_normalize_std = loc_normalize_std\n",
    "        self.use_preset('evaluate')\n",
    "\n",
    "    @property\n",
    "    def n_class(self):\n",
    "        # Total number of classes including the background.\n",
    "        return self.head.n_class\n",
    "\n",
    "    def forward(self, x, scale=1.):\n",
    "        \"\"\"Forward Faster R-CNN.\n",
    "        Scaling paramter :obj:`scale` is used by RPN to determine the\n",
    "        threshold to select small objects, which are going to be\n",
    "        rejected irrespective of their confidence scores.\n",
    "        Here are notations used.\n",
    "        * :math:`N` is the number of batch size\n",
    "        * :math:`R'` is the total number of RoIs produced across batches. \\\n",
    "            Given :math:`R_i` proposed RoIs from the :math:`i` th image, \\\n",
    "            :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n",
    "        * :math:`L` is the number of classes excluding the background.\n",
    "        Classes are ordered by the background, the first class, ..., and\n",
    "        the :math:`L` th class.\n",
    "        Args:\n",
    "            x (autograd.Variable): 4D image variable.\n",
    "            scale (float): Amount of scaling applied to the raw image\n",
    "                during preprocessing.\n",
    "        Returns:\n",
    "            Variable, Variable, array, array:\n",
    "            Returns tuple of four values listed below.\n",
    "            * **roi_cls_locs**: Offsets and scalings for the proposed RoIs. \\\n",
    "                Its shape is :math:`(R', (L + 1) \\\\times 4)`.\n",
    "            * **roi_scores**: Class predictions for the proposed RoIs. \\\n",
    "                Its shape is :math:`(R', L + 1)`.\n",
    "            * **rois**: RoIs proposed by RPN. Its shape is \\\n",
    "                :math:`(R', 4)`.\n",
    "            * **roi_indices**: Batch indices of RoIs. Its shape is \\\n",
    "                :math:`(R',)`.\n",
    "        \"\"\"\n",
    "        img_size = x.shape[2:]\n",
    "\n",
    "        h = self.extractor(x)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = \\\n",
    "            self.rpn(h, img_size, scale)\n",
    "        roi_cls_locs, roi_scores = self.head(\n",
    "            h, rois, roi_indices)\n",
    "        return roi_cls_locs, roi_scores, rois, roi_indices\n",
    "\n",
    "    def use_preset(self, preset):\n",
    "        \"\"\"Use the given preset during prediction.\n",
    "        This method changes values of :obj:`self.nms_thresh` and\n",
    "        :obj:`self.score_thresh`. These values are a threshold value\n",
    "        used for non maximum suppression and a threshold value\n",
    "        to discard low confidence proposals in :meth:`predict`,\n",
    "        respectively.\n",
    "        If the attributes need to be changed to something\n",
    "        other than the values provided in the presets, please modify\n",
    "        them by directly accessing the public attributes.\n",
    "        Args:\n",
    "            preset ({'visualize', 'evaluate'): A string to determine the\n",
    "                preset to use.\n",
    "        \"\"\"\n",
    "        if preset == 'visualize':\n",
    "            self.nms_thresh = 0.3\n",
    "            self.score_thresh = 0.7\n",
    "        elif preset == 'evaluate':\n",
    "            self.nms_thresh = 0.3\n",
    "            self.score_thresh = 0.05\n",
    "        else:\n",
    "            raise ValueError('preset must be visualize or evaluate')\n",
    "\n",
    "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
    "        bbox = list()\n",
    "        label = list()\n",
    "        score = list()\n",
    "        # skip cls_id = 0 because it is the background class\n",
    "        for l in range(1, self.n_class):\n",
    "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
    "            prob_l = raw_prob[:, l]\n",
    "            mask = prob_l > self.score_thresh\n",
    "            cls_bbox_l = cls_bbox_l[mask]\n",
    "            prob_l = prob_l[mask]\n",
    "            keep = non_maximum_suppression(\n",
    "                cp.array(cls_bbox_l), self.nms_thresh, prob_l)\n",
    "            keep = cp.asnumpy(keep)\n",
    "            bbox.append(cls_bbox_l[keep])\n",
    "            # The labels are in [0, self.n_class - 2].\n",
    "            label.append((l - 1) * np.ones((len(keep),)))\n",
    "            score.append(prob_l[keep])\n",
    "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
    "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
    "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
    "        return bbox, label, score\n",
    "\n",
    "    def predict(self, imgs,sizes=None,visualize=False):\n",
    "        \"\"\"Detect objects from images.\n",
    "        This method predicts objects for each image.\n",
    "        Args:\n",
    "            imgs (iterable of numpy.ndarray): Arrays holding images.\n",
    "                All images are in CHW and RGB format\n",
    "                and the range of their value is :math:`[0, 255]`.\n",
    "        Returns:\n",
    "           tuple of lists:\n",
    "           This method returns a tuple of three lists,\n",
    "           :obj:`(bboxes, labels, scores)`.\n",
    "           * **bboxes**: A list of float arrays of shape :math:`(R, 4)`, \\\n",
    "               where :math:`R` is the number of bounding boxes in a image. \\\n",
    "               Each bouding box is organized by \\\n",
    "               :math:`(y_{min}, x_{min}, y_{max}, x_{max})` \\\n",
    "               in the second axis.\n",
    "           * **labels** : A list of integer arrays of shape :math:`(R,)`. \\\n",
    "               Each value indicates the class of the bounding box. \\\n",
    "               Values are in range :math:`[0, L - 1]`, where :math:`L` is the \\\n",
    "               number of the foreground classes.\n",
    "           * **scores** : A list of float arrays of shape :math:`(R,)`. \\\n",
    "               Each value indicates how confident the prediction is.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        if visualize:\n",
    "            self.use_preset('visualize')\n",
    "            prepared_imgs = list()\n",
    "            sizes = list()\n",
    "            for img in imgs:\n",
    "                size = img.shape[1:]\n",
    "                img = preprocess(at.tonumpy(img))\n",
    "                prepared_imgs.append(img)\n",
    "                sizes.append(size)\n",
    "        else:\n",
    "             prepared_imgs = imgs \n",
    "        bboxes = list()\n",
    "        labels = list()\n",
    "        scores = list()\n",
    "        for img, size in zip(prepared_imgs, sizes):\n",
    "            img = t.autograd.Variable(at.totensor(img).float()[None], volatile=True)\n",
    "            scale = img.shape[3] / size[1]\n",
    "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale)\n",
    "            # We are assuming that batch size is 1.\n",
    "            roi_score = roi_scores.data\n",
    "            roi_cls_loc = roi_cls_loc.data\n",
    "            roi = at.totensor(rois) / scale\n",
    "\n",
    "            # Convert predictions to bounding boxes in image coordinates.\n",
    "            # Bounding boxes are scaled to the scale of the input images.\n",
    "            mean = t.Tensor(self.loc_normalize_mean).cuda(). \\\n",
    "                repeat(self.n_class)[None]\n",
    "            std = t.Tensor(self.loc_normalize_std).cuda(). \\\n",
    "                repeat(self.n_class)[None]\n",
    "\n",
    "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
    "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
    "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
    "            cls_bbox = loc2bbox(at.tonumpy(roi).reshape((-1, 4)),\n",
    "                                at.tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
    "            cls_bbox = at.totensor(cls_bbox)\n",
    "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
    "            # clip bounding box\n",
    "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
    "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
    "\n",
    "            prob = at.tonumpy(F.softmax(at.tovariable(roi_score), dim=1))\n",
    "\n",
    "            raw_cls_bbox = at.tonumpy(cls_bbox)\n",
    "            raw_prob = at.tonumpy(prob)\n",
    "\n",
    "            bbox, label, score = self._suppress(raw_cls_bbox, raw_prob)\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        self.use_preset('evaluate')\n",
    "        self.train()\n",
    "        return bboxes, labels, scores\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\"\n",
    "        return optimizer, It could be overwriten if you want to specify \n",
    "        special optimizer\n",
    "        \"\"\"\n",
    "        lr = opt.lr\n",
    "        params = []\n",
    "        for key, value in dict(self.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if 'bias' in key:\n",
    "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
    "                else:\n",
    "                    params += [{'params': [value], 'lr': lr, 'weight_decay': opt.weight_decay}]\n",
    "        if opt.use_adam:\n",
    "            self.optimizer = t.optim.Adam(params)\n",
    "        else:\n",
    "            self.optimizer = t.optim.SGD(params, momentum=0.9)\n",
    "        return self.optimizer\n",
    "\n",
    "    def scale_lr(self, decay=0.1):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] *= decay\n",
    "return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
